{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Natural Language Processing for Chatbots\n",
    "\n",
    "**spaCy**: \"an open-source software library for advanced NLP, written in Python and Cython, built by Matthew Honnibal. It provides intuitive APIs to access its methods trained by deep learning models\".\n",
    "\n",
    "More @ [spaCy website](https://spacy.io/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.11'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts-of-speech (POS) tagging\n",
    "\n",
    "\"a process where you read some text and assign parts of speech to each word or token, such as noun, verb, adjective, etc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I \t POS: PRON\n",
      "Text: am \t POS: VERB\n",
      "Text: learning \t POS: VERB\n",
      "Text: how \t POS: ADV\n",
      "Text: to \t POS: PART\n",
      "Text: build \t POS: VERB\n",
      "Text: chatbots \t POS: NOUN\n"
     ]
    }
   ],
   "source": [
    "# Loads spacy en model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Creates doc object\n",
    "doc = nlp(u'I am learning how to build chatbots')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Text: {token.text} \\t POS: {token.pos_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I \t POS: PRON\n",
      "Text: am \t POS: VERB\n",
      "Text: going \t POS: VERB\n",
      "Text: to \t POS: ADP\n",
      "Text: London \t POS: PROPN\n",
      "Text: next \t POS: ADJ\n",
      "Text: week \t POS: NOUN\n",
      "Text: for \t POS: ADP\n",
      "Text: a \t POS: DET\n",
      "Text: meeting \t POS: NOUN\n",
      "Text: . \t POS: PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am going to London next week for a meeting.')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Text: {token.text} \\t POS: {token.pos_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token(token):\n",
    "    print(f'Text: {token.text}\\tLemma: {token.lemma_}\\tPOS: {token.pos_}')\n",
    "    print(f'Tag: {token.tag_}\\tDependency: {token.dep_}\\tShape: {token.shape_}')\n",
    "    print(f'Is alpha-numeric? {token.is_alpha}\\tIs stopword? {token.is_stop}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Google\tLemma: google\tPOS: PROPN\n",
      "Tag: NNP\tDependency: compound\tShape: Xxxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: release\tLemma: release\tPOS: NOUN\n",
      "Tag: NN\tDependency: nmod\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: \"\tLemma: \"\tPOS: PUNCT\n",
      "Tag: ``\tDependency: punct\tShape: \"\n",
      "Is alpha-numeric? False\tIs stopword? False\n",
      "\n",
      "Text: Move\tLemma: move\tPOS: PROPN\n",
      "Tag: NNP\tDependency: nmod\tShape: Xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: Mirror\tLemma: mirror\tPOS: PROPN\n",
      "Tag: NNP\tDependency: nmod\tShape: Xxxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: \"\tLemma: \"\tPOS: PUNCT\n",
      "Tag: ''\tDependency: punct\tShape: \"\n",
      "Is alpha-numeric? False\tIs stopword? False\n",
      "\n",
      "Text: AI\tLemma: ai\tPOS: PROPN\n",
      "Tag: NNP\tDependency: compound\tShape: XX\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: experiment\tLemma: experiment\tPOS: NOUN\n",
      "Tag: NN\tDependency: ROOT\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: that\tLemma: that\tPOS: ADJ\n",
      "Tag: WDT\tDependency: nsubj\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? True\n",
      "\n",
      "Text: matches\tLemma: match\tPOS: VERB\n",
      "Tag: VBZ\tDependency: relcl\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: your\tLemma: -PRON-\tPOS: ADJ\n",
      "Tag: PRP$\tDependency: poss\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? True\n",
      "\n",
      "Text: pose\tLemma: pose\tPOS: NOUN\n",
      "Tag: NN\tDependency: dobj\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: from\tLemma: from\tPOS: ADP\n",
      "Tag: IN\tDependency: prep\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? True\n",
      "\n",
      "Text: 80,000\tLemma: 80,000\tPOS: NUM\n",
      "Tag: CD\tDependency: nummod\tShape: dd,ddd\n",
      "Is alpha-numeric? False\tIs stopword? False\n",
      "\n",
      "Text: images\tLemma: image\tPOS: NOUN\n",
      "Tag: NNS\tDependency: pobj\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Google release \"Move Mirror\" AI experiment that matches your pose from 80,000 images')\n",
    "\n",
    "for token in doc:\n",
    "    print_token(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token attributes:\n",
    "\n",
    "<img src='./IMG/token-attrs.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS attributes:\n",
    "\n",
    "<img src='./IMG/pos-attrs.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming: \"reducing inflected words to their word stem, base form\".\n",
    "- Ex.: saying -> say.\n",
    "\n",
    "Lemmatization: \"algorithmic process of determining the *lemma* of a word based on its intended meaning\".\n",
    "- Ex.: walk, walks, walked, walking -> walk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change in API!**\n",
    "\n",
    "See [this](https://stackoverflow.com/questions/58779371/importerror-cannot-import-name-lemma-index-from-spacy-lang-en) and [this](https://spacy.io/usage/v2-2#migrating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chuckle']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('chuckling', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blaze']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('blazing', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fast']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('fastest', 'ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition\n",
    "\n",
    "Named-Entity Recognition (NER): \"process of finding and classifying [named entities](https://en.wikipedia.org/wiki/Named_entity) existing in the given text into pre-defined categories\".\n",
    "- \"hugely dependent on the knowledge base used to train the NE extraction algorithm\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Google\tLabel: ORG\n",
      "Text: Mountain View\tLabel: GPE\n",
      "Text: California\tLabel: GPE\n",
      "Text: 109.65 billion US dollars\tLabel: MONEY\n"
     ]
    }
   ],
   "source": [
    "my_string = 'Google has its headquarters in Mountain View, \\\n",
    "California having revenue amounted to 109.65 billion US dollars'\n",
    "\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Text: {ent.text}\\tLabel: {ent.label_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Mark Zuckerberg\tLabel: PERSON\n",
      "Text: May 14, 1984\tLabel: DATE\n",
      "Text: New York\tLabel: GPE\n",
      "Text: American\tLabel: NORP\n",
      "Text: Facebook\tLabel: ORG\n"
     ]
    }
   ],
   "source": [
    "my_string= 'Mark Zuckerberg born May 14, 1984 in New York \\\n",
    "is an American technology entrepreneur and philanthropist \\\n",
    "best known for co-founding and leading Facebook as its chairman and CEO.'\n",
    "\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Text: {ent.text}\\tLabel: {ent.label_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 9:00 AM\tLabel: TIME\n",
      "Text: 90%\tLabel: PERCENT\n"
     ]
    }
   ],
   "source": [
    "my_string = 'I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things.'\n",
    "\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Text: {ent.text}\\tLabel: {ent.label_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity types:\n",
    "\n",
    "<img src='./IMG/entity-types.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Whenever we intend to build a conversational agent or chatbot in simple terms, we always have a domain in mind.\"\n",
    "- \"By finding out the entity in the question, one can get a fair idea of the context in which the question was asked.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine Dragons ORG\n"
     ]
    }
   ],
   "source": [
    "my_string1 = 'Imagine Dragons are the best band.'\n",
    "my_string2 = 'Imagine dragons come and take over the city.'\n",
    "\n",
    "doc1 = nlp(my_string1)\n",
    "doc2 = nlp(my_string2)\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "<img src='./IMG/stopwords.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'might', 'the', 'full', 'therefore', 'that', 'if', 'here', 'such', 'those', 'back', 'done', 'whence', 'to', 'ten', 'there', 'amount', 'whether', 'really', 'hereafter', 'yours', 'can', 'fifteen', 'already', 'no', 'wherein', 'next', 'or', 'in', 'he', 'own', 'nobody', 'may', 'noone', 'now', 'three', 'via', 'during', 'through', 'anywhere', 'show', 'eight', 'various', 'with', 'as', 'any', 'anyway', 'doing', 'must', 'none', 're', 'somehow', 'first', 'across', 'namely', 'serious', 'she', 'us', 'your', 'please', 'further', 'never', 'someone', 'sometimes', 'thereupon', 'under', 'five', 'whereupon', 'one', 'until', 'could', 'yourself', 'meanwhile', 'before', 'so', 'everything', 'not', 'neither', 'out', 'yet', 'themselves', 'anyone', 'this', 'herein', 'how', 'its', 'my', 'onto', 'thereafter', 'you', 'but', 'using', 'sixty', 'thus', 'just', 'rather', 'whenever', 'among', 'every', 'fifty', 'give', 'even', 'latterly', 'mine', 'ours', 'had', 'elsewhere', 'twelve', 'whatever', 'have', 'mostly', 'only', 'their', 'a', 'towards', 'everywhere', 'whereafter', 'however', 'me', 'hereby', 'bottom', 'else', 'go', 'least', 'quite', 'much', 'nothing', 'most', 'because', 'per', 'though', 'top', 'should', 'who', 'seems', 'became', 'eleven', 'latter', 'somewhere', 'therein', 'become', 'was', 'within', 'do', 'beforehand', 'whither', 'get', 'why', 'each', 'also', 'behind', 'his', 'nevertheless', 'an', 'hereupon', 'of', 'sometime', 'up', 'well', 'from', 'has', 'less', 'some', 'what', 'thru', 'empty', 'does', 'few', 'move', 'both', 'anything', 'and', 'itself', 'on', 'throughout', 'many', 'him', 'perhaps', 'name', 'ca', 'everyone', 'front', 'nine', 'again', 'see', 'something', 'although', 'keep', 'upon', 'whoever', 'whereby', 'will', 'nor', 'they', 'these', 'herself', 'than', 'we', 'due', 'four', 'toward', 'without', 'third', 'whose', 'between', 'otherwise', 'around', 'made', 'wherever', 'hence', 'together', 'for', 'formerly', 'call', 'into', 'amongst', 'indeed', 'regarding', 'when', 'been', 'whereas', 'after', 'make', 'down', 'alone', 'then', 'moreover', 'below', 'is', 'side', 'above', 'all', 'be', 'often', 'over', 'which', 'others', 'except', 'against', 'at', 'seeming', 'several', 'whole', 'either', 'would', 'former', 'always', 'yourselves', 'are', 'unless', 'about', 'part', 'more', 'used', 'same', 'twenty', 'afterwards', 'enough', 'were', 'another', 'by', 'ever', 'cannot', 'since', 'her', 'becoming', 'them', 'becomes', 'i', 'two', 'myself', 'too', 'six', 'hers', 'nowhere', 'himself', 'very', 'other', 'anyhow', 'besides', 'beyond', 'where', 'hundred', 'forty', 'take', 'almost', 'put', 'seemed', 'being', 'once', 'thence', 'say', 'our', 'last', 'seem', 'did', 'along', 'off', 'beside', 'still', 'thereby', 'ourselves', 'while', 'am', 'whom', 'it'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['hello'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing\n",
    "\n",
    "\"gives you a parsed tree that explains the parent-child relationship between the words or phrases and is independent of the order in which words occur.\"\n",
    "\n",
    "**Ancestors**: \"the rightmost token of this token's syntactic descendants\".\n",
    "\n",
    "**Children**: \"immediate syntactic dependents of the token.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Book me a flight from Bangalore to Goa')\n",
    "blr, goa = doc[5], doc[7]\n",
    "\n",
    "list(blr.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancestors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from [flight, Book]\n"
     ]
    }
   ],
   "source": [
    "print(doc[4], list(doc[4].ancestors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_ancestor(doc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].is_ancestor(doc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking of table belongs to restaurant\n",
      "Booking of taxi belongs to hotel\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Book a table at the restaurant and the taxi to the hotel')\n",
    "tasks = doc[2], doc[8] #(table, taxi)\n",
    "tasks_target = doc[5], doc[11] #(restaurant, hotel)\n",
    "\n",
    "for task in tasks_target:\n",
    "    for tok in task.ancestors:\n",
    "        if tok in tasks:\n",
    "            print(\"Booking of {} belongs to {}\".format(tok, task))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a, from, to]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Book me a flight from Bangalore to Goa')\n",
    "\n",
    "list(doc[3].children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "buffer source array is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5ef6afad3856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Book a table at the restaurant and the taxi to the hotel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/nlp-studies/.env/lib/python3.8/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(docs, style, page, minify, options, manual, port)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mwsgiref\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     render(docs, style=style, page=page, minify=minify, options=options,\n\u001b[0m\u001b[1;32m     59\u001b[0m            manual=manual)\n\u001b[1;32m     60\u001b[0m     \u001b[0mhttpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0.0.0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/nlp-studies/.env/lib/python3.8/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0m_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/nlp-studies/.env/lib/python3.8/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0m_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parsed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/nlp-studies/.env/lib/python3.8/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36mparse_deps\u001b[0;34m(orig_doc, options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGenerated\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0mparse\u001b[0m \u001b[0mkeyed\u001b[0m \u001b[0mby\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0muser_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.from_bytes\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Projects/nlp-studies/.env/lib/python3.8/site-packages/spacy/tokens/doc.cpython-38-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mView.MemoryView.memoryview_cwrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Projects/nlp-studies/.env/lib/python3.8/site-packages/spacy/tokens/doc.cpython-38-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mView.MemoryView.memoryview.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: buffer source array is read-only"
     ]
    }
   ],
   "source": [
    "doc = nlp('Book a table at the restaurant and the taxi to the hotel')\n",
    "displacy.serve(doc, style='dep')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
