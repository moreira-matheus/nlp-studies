{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Natural Language Processing for Chatbots\n",
    "\n",
    "**spaCy**: \"an open-source software library for advanced NLP, written in Python and Cython, built by Matthew Honnibal. It provides intuitive APIs to access its methods trained by deep learning models\".\n",
    "\n",
    "More @ [spaCy website](https://spacy.io/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.11'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts-of-speech (POS) tagging\n",
    "\n",
    "\"a process where you read some text and assign parts of speech to each word or token, such as noun, verb, adjective, etc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I \t POS: PRON\n",
      "Text: am \t POS: VERB\n",
      "Text: learning \t POS: VERB\n",
      "Text: how \t POS: ADV\n",
      "Text: to \t POS: PART\n",
      "Text: build \t POS: VERB\n",
      "Text: chatbots \t POS: NOUN\n"
     ]
    }
   ],
   "source": [
    "# Loads spacy en model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Creates doc object\n",
    "doc = nlp(u'I am learning how to build chatbots')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Text: {token.text} \\t POS: {token.pos_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I \t POS: PRON\n",
      "Text: am \t POS: VERB\n",
      "Text: going \t POS: VERB\n",
      "Text: to \t POS: ADP\n",
      "Text: London \t POS: PROPN\n",
      "Text: next \t POS: ADJ\n",
      "Text: week \t POS: NOUN\n",
      "Text: for \t POS: ADP\n",
      "Text: a \t POS: DET\n",
      "Text: meeting \t POS: NOUN\n",
      "Text: . \t POS: PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am going to London next week for a meeting.')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Text: {token.text} \\t POS: {token.pos_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token(token):\n",
    "    print(f'Text: {token.text}\\tLemma: {token.lemma_}\\tPOS: {token.pos_}')\n",
    "    print(f'Tag: {token.tag_}\\tDependency: {token.dep_}\\tShape: {token.shape_}')\n",
    "    print(f'Is alpha-numeric? {token.is_alpha}\\tIs stopword? {token.is_stop}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Google\tLemma: google\tPOS: PROPN\n",
      "Tag: NNP\tDependency: compound\tShape: Xxxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: release\tLemma: release\tPOS: NOUN\n",
      "Tag: NN\tDependency: nmod\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: \"\tLemma: \"\tPOS: PUNCT\n",
      "Tag: ``\tDependency: punct\tShape: \"\n",
      "Is alpha-numeric? False\tIs stopword? False\n",
      "\n",
      "Text: Move\tLemma: move\tPOS: PROPN\n",
      "Tag: NNP\tDependency: nmod\tShape: Xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: Mirror\tLemma: mirror\tPOS: PROPN\n",
      "Tag: NNP\tDependency: nmod\tShape: Xxxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: \"\tLemma: \"\tPOS: PUNCT\n",
      "Tag: ''\tDependency: punct\tShape: \"\n",
      "Is alpha-numeric? False\tIs stopword? False\n",
      "\n",
      "Text: AI\tLemma: ai\tPOS: PROPN\n",
      "Tag: NNP\tDependency: compound\tShape: XX\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: experiment\tLemma: experiment\tPOS: NOUN\n",
      "Tag: NN\tDependency: ROOT\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: that\tLemma: that\tPOS: ADJ\n",
      "Tag: WDT\tDependency: nsubj\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? True\n",
      "\n",
      "Text: matches\tLemma: match\tPOS: VERB\n",
      "Tag: VBZ\tDependency: relcl\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: your\tLemma: -PRON-\tPOS: ADJ\n",
      "Tag: PRP$\tDependency: poss\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? True\n",
      "\n",
      "Text: pose\tLemma: pose\tPOS: NOUN\n",
      "Tag: NN\tDependency: dobj\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n",
      "Text: from\tLemma: from\tPOS: ADP\n",
      "Tag: IN\tDependency: prep\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? True\n",
      "\n",
      "Text: 80,000\tLemma: 80,000\tPOS: NUM\n",
      "Tag: CD\tDependency: nummod\tShape: dd,ddd\n",
      "Is alpha-numeric? False\tIs stopword? False\n",
      "\n",
      "Text: images\tLemma: image\tPOS: NOUN\n",
      "Tag: NNS\tDependency: pobj\tShape: xxxx\n",
      "Is alpha-numeric? True\tIs stopword? False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Google release \"Move Mirror\" AI experiment that matches your pose from 80,000 images')\n",
    "\n",
    "for token in doc:\n",
    "    print_token(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token attributes:\n",
    "\n",
    "<img src='./IMG/token-attrs.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS attributes:\n",
    "\n",
    "<img src='./IMG/pos-attrs.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming: \"reducing inflected words to their word stem, base form\".\n",
    "- Ex.: saying -> say.\n",
    "\n",
    "Lemmatization: \"algorithmic process of determining the *lemma* of a word based on its intended meaning\".\n",
    "- Ex.: walk, walks, walked, walking -> walk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change in API!**\n",
    "\n",
    "See [this](https://stackoverflow.com/questions/58779371/importerror-cannot-import-name-lemma-index-from-spacy-lang-en) and [this](https://spacy.io/usage/v2-2#migrating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chuckle']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('chuckling', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blaze']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('blazing', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fast']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('fastest', 'ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition\n",
    "\n",
    "Named-Entity Recognition (NER): \"process of finding and classifying [named entities](https://en.wikipedia.org/wiki/Named_entity) existing in the given text into pre-defined categories\".\n",
    "- \"hugely dependent on the knowledge base used to train the NE extraction algorithm\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Google\tLabel: ORG\n",
      "Text: Mountain View\tLabel: GPE\n",
      "Text: California\tLabel: GPE\n",
      "Text: 109.65 billion US dollars\tLabel: MONEY\n"
     ]
    }
   ],
   "source": [
    "my_string = 'Google has its headquarters in Mountain View, \\\n",
    "California having revenue amounted to 109.65 billion US dollars'\n",
    "\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Text: {ent.text}\\tLabel: {ent.label_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Mark Zuckerberg\tLabel: PERSON\n",
      "Text: May 14, 1984\tLabel: DATE\n",
      "Text: New York\tLabel: GPE\n",
      "Text: American\tLabel: NORP\n",
      "Text: Facebook\tLabel: ORG\n"
     ]
    }
   ],
   "source": [
    "my_string= 'Mark Zuckerberg born May 14, 1984 in New York \\\n",
    "is an American technology entrepreneur and philanthropist \\\n",
    "best known for co-founding and leading Facebook as its chairman and CEO.'\n",
    "\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Text: {ent.text}\\tLabel: {ent.label_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 9:00 AM\tLabel: TIME\n",
      "Text: 90%\tLabel: PERCENT\n"
     ]
    }
   ],
   "source": [
    "my_string = 'I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things.'\n",
    "\n",
    "doc = nlp(my_string)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Text: {ent.text}\\tLabel: {ent.label_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity types:\n",
    "\n",
    "<img src='./IMG/entity-types.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Whenever we intend to build a conversational agent or chatbot in simple terms, we always have a domain in mind.\"\n",
    "- \"By finding out the entity in the question, one can get a fair idea of the context in which the question was asked.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine Dragons ORG\n"
     ]
    }
   ],
   "source": [
    "my_string1 = 'Imagine Dragons are the best band.'\n",
    "my_string2 = 'Imagine dragons come and take over the city.'\n",
    "\n",
    "doc1 = nlp(my_string1)\n",
    "doc2 = nlp(my_string2)\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "<img src='./IMG/stopwords.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from', 'thereafter', 'anywhere', 'around', 'nothing', 'hence', 'take', 'upon', 'how', 'go', 'too', 'further', 'here', 'is', 'becoming', 'cannot', 'regarding', 'toward', 'out', 'seemed', 'although', 'several', 'nine', 'even', 'does', 'thereupon', 'seems', 'been', 'his', 'behind', 'into', 'neither', 'anyway', 'thus', 'throughout', 'well', 'anyhow', 'still', 'our', 'above', 'become', 'she', 'thru', 'amount', 'will', 'whether', 'becomes', 'without', 'many', 'such', 'only', 'with', 'say', 'whereas', 'did', 'meanwhile', 'except', 'four', 'hereupon', 'ever', 'off', 'over', 'him', 'another', 'himself', 'they', 'after', 'among', 'own', 'before', 'some', 'a', 'always', 'not', 'should', 'none', 'herself', 'else', 'along', 'please', 'first', 'yourself', 'has', 'something', 'quite', 'through', 'hereafter', 'whose', 'if', 'more', 'put', 'do', 'anyone', 'enough', 'much', 'then', 'whatever', 'get', 'about', 'me', 'once', 'show', 'latterly', 'for', 'towards', 'keep', 'yours', 'against', 'the', 'an', 'can', 'others', 'thence', 'thereby', 'both', 'them', 'wherever', 'no', 'eleven', 'also', 'back', 'could', 'nowhere', 'sometimes', 'when', 'would', 'either', 'hereby', 'seeming', 'perhaps', 'part', 'nobody', 'formerly', 'seem', 'must', 'because', 'but', 'besides', 'your', 'on', 'those', 'who', 'everyone', 'bottom', 'namely', 'whole', 'same', 'few', 'to', 'onto', 'per', 'sixty', 'each', 'next', 'it', 'six', 'below', 'empty', 'hers', 'noone', 'so', 'someone', 'these', 'may', 'were', 'though', 'everything', 'being', 'where', 'make', 'am', 'call', 'beside', 'might', 'give', 'or', 'he', 'twenty', 'what', 'wherein', 'fifty', 'most', 'top', 'whom', 'during', 'between', 'all', 'sometime', 'while', 'whoever', 'my', 'yourselves', 'of', 'doing', 'as', 'why', 'beyond', 'via', 'two', 'front', 'elsewhere', 'myself', 'twelve', 'up', 'used', 'whence', 'rather', 'five', 'hundred', 'its', 'latter', 'amongst', 'made', 'ourselves', 'until', 'nevertheless', 'her', 'mine', 'side', 'at', 'one', 'that', 'under', 'across', 'other', 'due', 'herein', 'unless', 'now', 'whereafter', 'you', 'by', 'i', 'almost', 'are', 'fifteen', 'serious', 'down', 'forty', 'this', 'yet', 'move', 'eight', 'since', 'somehow', 'their', 'themselves', 'and', 'therefore', 'whereupon', 'using', 'ours', 'there', 'three', 're', 'very', 'we', 'last', 'whereby', 'which', 'whither', 'together', 'just', 'really', 'full', 'indeed', 'every', 'otherwise', 'became', 'beforehand', 'less', 'than', 'whenever', 'itself', 'again', 'alone', 'somewhere', 'however', 'us', 'therein', 'various', 'in', 'done', 'had', 'never', 'moreover', 'least', 'nor', 'afterwards', 'ca', 'everywhere', 'mostly', 'ten', 'any', 'see', 'was', 'be', 'often', 'third', 'name', 'have', 'former', 'already', 'within', 'anything'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['hello'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing\n",
    "\n",
    "\"gives you a parsed tree that explains the parent-child relationship between the words or phrases and is independent of the order in which words occur.\"\n",
    "\n",
    "**Ancestors**: \"the rightmost token of this token's syntactic descendants\".\n",
    "\n",
    "**Children**: \"immediate syntactic dependents of the token.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Book me a flight from Bangalore to Goa')\n",
    "blr, goa = doc[5], doc[7]\n",
    "\n",
    "list(blr.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ancestors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from [flight, Book]\n"
     ]
    }
   ],
   "source": [
    "print(doc[4], list(doc[4].ancestors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_ancestor(doc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].is_ancestor(doc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking of table belongs to restaurant\n",
      "Booking of taxi belongs to hotel\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Book a table at the restaurant and the taxi to the hotel')\n",
    "tasks = doc[2], doc[8] #(table, taxi)\n",
    "tasks_target = doc[5], doc[11] #(restaurant, hotel)\n",
    "\n",
    "for task in tasks_target:\n",
    "    for tok in task.ancestors:\n",
    "        if tok in tasks:\n",
    "            print(\"Booking of {} belongs to {}\".format(tok, task))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Children:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a, from, to]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Book me a flight from Bangalore to Goa')\n",
    "\n",
    "list(doc[3].children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK LATER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# doc = nlp('Book a table at the restaurant and the taxi to the hotel')\n",
    "# displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is referring Berlin to visit\n",
      "User is referring Lubeck to stay\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('What are some places to visit in Berlin and stay in Lubeck?')\n",
    "\n",
    "places = [doc[7], doc[11]] #[Berlin, Lubeck]\n",
    "actions = [doc[5], doc[9]] #[visit, stay]\n",
    "\n",
    "for place in places:\n",
    "    for tok in place.ancestors:\n",
    "        if tok in actions:\n",
    "            print(\"User is referring {} to {}\".format(place, tok))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun chunks\n",
    "\n",
    "Noun chunks: \"flat phrases that have a noun as their head\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Boston Dynamics, thousands, robot dogs]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Boston Dynamics is gearing up to produce thousands of robot dogs')\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Deep learning\n",
      "Root text:  learning\n",
      "Root dependency:  nsubj\n",
      "Root head text:  cracks\n",
      "\n",
      "Text:  the code\n",
      "Root text:  code\n",
      "Root dependency:  dobj\n",
      "Root head text:  cracks\n",
      "\n",
      "Text:  messenger RNAs\n",
      "Root text:  RNAs\n",
      "Root dependency:  pobj\n",
      "Root head text:  of\n",
      "\n",
      "Text:  protein-coding potential\n",
      "Root text:  potential\n",
      "Root dependency:  conj\n",
      "Root head text:  RNAs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Deep learning cracks the code of messenger RNAs and protein-coding potential')\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('Text: ', chunk.text)\n",
    "    print('Root text: ', chunk.root.text)\n",
    "    print('Root dependency: ', chunk.root.dep_)\n",
    "    print('Root head text: ', chunk.root.head.text)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun chunks attributes:\n",
    "\n",
    "<img src='./IMG/noun-chunks-attrs.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similarity\n",
    "\n",
    "\"spaCy uses high-quality word vectors to find similarity between two words using [GloVe](https://nlp.stanford.edu/projects/glove/) algorithm\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How [-0.29742622  0.73939687 -0.04001444  0.44034058  2.8967497 ]\n",
      "are [-0.23435101 -1.6145048   1.0197463   0.99281645  0.2822714 ]\n",
      "you [ 0.10252154 -3.5647113   2.482279    4.2824993   3.5902457 ]\n",
      "doing [-0.6240917  -2.0210214  -0.91014886  2.7051926   4.1892524 ]\n",
      "today [ 3.5409102  -0.6218591   2.6274276   2.0504882   0.20191938]\n",
      "? [ 2.8914993  -0.25079137  3.3764176   1.6942683   1.9849057 ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('How are you doing today?')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.vector[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7879069991402569\n",
      "0.4193426329044165\n"
     ]
    }
   ],
   "source": [
    "hello_doc = nlp('hello')\n",
    "hi_doc = nlp('hi')\n",
    "hella_doc = nlp('hella')\n",
    "\n",
    "print(hello_doc.similarity(hi_doc))\n",
    "print(hello_doc.similarity(hella_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785019065199066"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GoT_str1 = nlp('When will next season of Game of Thrones be releasing?')\n",
    "GoT_str2 = nlp('Game of Thrones next season release date?')\n",
    "\n",
    "GoT_str1.similarity(GoT_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n",
      "Word car is 71% similar to word truck\n",
      "Word car is 24% similar to word google\n",
      "Word truck is 71% similar to word car\n",
      "Word truck is 100% similar to word truck\n",
      "Word truck is 36% similar to word google\n",
      "Word google is 24% similar to word car\n",
      "Word google is 36% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    }
   ],
   "source": [
    "example_doc = nlp(u\"car truck google\")\n",
    "\n",
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2) * 100)\n",
    "        print('Word {} is {}% similar to word {}'.format(t1.text, similarity_perc, t2.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization: \"split[ting] a text into meaningful segments\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit\n",
      "is\n",
      "the\n",
      "impending\n",
      "withdrawal\n",
      "of\n",
      "the\n",
      "U.K.\n",
      "from\n",
      "the\n",
      "European\n",
      "Union\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Brexit is the impending withdrawal of the U.K. from the European Union.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Book me a metro from Airport Station to Hong Kong Station.\"\n",
    "sentence2 = \"Book me a cab to Hong Kong Airport from AsiaWorld-Expo.\"\n",
    "\n",
    "from_to = re.compile('.* from (.*) to (.*)')\n",
    "to_from = re.compile('.* to (.*) from (.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_from pattern matched correctly. Printing values\n",
      "\n",
      "From: AsiaWorld-Expo., To: Hong Kong Airport\n"
     ]
    }
   ],
   "source": [
    "from_to_match = from_to.match(sentence2)\n",
    "to_from_match = to_from.match(sentence2)\n",
    "\n",
    "if from_to_match and from_to_match.groups():\n",
    "    _from = from_to_match.groups()[0]\n",
    "    _to = from_to_match.groups()[1]\n",
    "    print(\"from_to pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))\n",
    "\n",
    "elif to_from_match and to_from_match.groups():\n",
    "    _to = to_from_match.groups()[0]\n",
    "    _from = to_from_match.groups()[1]\n",
    "    print(\"to_from pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_to pattern matched correctly. Printing values\n",
      "\n",
      "From: Airport Station, To: Hong Kong Station.\n"
     ]
    }
   ],
   "source": [
    "from_to_match = from_to.match(sentence1)\n",
    "to_from_match = to_from.match(sentence1)\n",
    "\n",
    "if from_to_match and from_to_match.groups():\n",
    "    _from = from_to_match.groups()[0]\n",
    "    _to = from_to_match.groups()[1]\n",
    "    print(\"from_to pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))\n",
    "\n",
    "elif to_from_match and to_from_match.groups():\n",
    "    _to = to_from_match.groups()[0]\n",
    "    _from = to_from_match.groups()[1]\n",
    "    print(\"to_from pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
