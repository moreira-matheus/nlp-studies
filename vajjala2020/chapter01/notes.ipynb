{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLP: A Primer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Tasks\n",
    "\n",
    "**Language modeling**: predicting (assessing probability of) the next word in a sentence, based on history. Applied to speech recognition, OCR recognition, handwriting recognition etc.\n",
    "\n",
    "**Text classification**: bucketing text elements into a set of categories. Applied to spam identification and sentiment analysis.\n",
    "\n",
    "**Information extraction**: finding relevant documents out of a collection.\n",
    "\n",
    "**Conversational agent**: building systems able to converse in natural language. E.g., Alexa and Siri.\n",
    "\n",
    "**Text summarization**: producing short, meaningful summaries of long documents.\n",
    "\n",
    "**Question answering**: building systems able to answer questions asked in human language.\n",
    "\n",
    "**Machine translation**: translating a documento from a language to another, automatically.\n",
    "\n",
    "**Topic modeling**: extracting main topics out of a collection of documents.\n",
    "\n",
    "### Language\n",
    "\n",
    "Four major building blocks: <u>phonemes</u>, <u>morphemes and lexemes</u>, <u>syntax</u>, and <u>context</u>.\n",
    "\n",
    "|      **Block**      |     **Concerns**    |                    **Applications**                    |\n",
    "|:-------------------:|:-------------------:|:------------------------------------------------------:|\n",
    "| Context             | Meaning             | Summarization, Topic Modeling, Sentiment Analysis      |\n",
    "| Syntax              | Phrases & Sentences | Parsing, Entity Extraction, Relation Extraction        |\n",
    "| Morphemes & Lexemes | Words               | Tokenization, Word Embeddings, POS Tagging             |\n",
    "| Phonemes            | Speech & Sounds     | Speech to Text, Speaker Identification, Text to Speech |\n",
    "\n",
    "**Phonemes**: smallest unit of sound. E.g., `/k/` as in *cat*.\n",
    "\n",
    "**Morphemes and lexemes**:\n",
    "\n",
    "- Morphemes: smallest meaningful unit of language. E.g., *unbreakable* -> *un + break + able*.\n",
    "\n",
    "- Lexemes: structural variations of morphemes. E.g., *run* and *running* are related to the same lexeme.\n",
    "\n",
    "**Syntax**: set of grammar rules to build sentences.\n",
    "\n",
    "- Level 3 (sentence): *The girl laughed at the monkey*.\n",
    "\n",
    "- Level 2 (phrases):\n",
    "    - Noun phrase (NP): *The girl*;\n",
    "    - Verb phrase (VP): *laughed at the monkey*.\n",
    "\n",
    "- Level 1 (parts of speech):\n",
    "    - Determinant (Det): *The*;\n",
    "    - Noun (N): *girl*;\n",
    "    - Verb (V): *laughed*;\n",
    "    - Preposition: *at*;\n",
    "    - Determinant (Det): *the*;\n",
    "    - Noun: *monkey*.\n",
    "\n",
    "- Level 0 (words): *The*, *girl*, *laughed*, *at*, *the*, *monkey*.\n",
    "\n",
    "**Context**: interactions between the elements of the language that convey meaning. Usually composed of *semantics* and *pragmatics*.\n",
    "\n",
    "- Semantics: meaning of the words without external contexr.\n",
    "\n",
    "- Pragmatics: takes world knowledge and external context into consideration.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- Ambiguity\n",
    "- Common knowledge\n",
    "- Creativity\n",
    "- Language diversity\n",
    "\n",
    "### Approaches\n",
    "\n",
    "**Heuristics-based NLP**: required expertise in the domain to formulate the rules. Tools: [`regex`](https://docs.python.org/3/library/re.html), [`pregex`](https://github.com/insperatum/pregex), [context-free grammars](https://hackage.haskell.org/package/Earley), [JAPE](https://en.wikipedia.org/wiki/JAPE_(linguistics)).\n",
    "\n",
    "**Machine Learning for NLP**:\n",
    "\n",
    "- Naive Bayes: algorithm for classification, based on Bayes' Theorem; calculates the probability of observing a label, given the input data; assumes the features are independent.\n",
    "\n",
    "- Support Vector Machine: algorithm for classification, aimed at learning a (linear or non-linear) decision boundary, so that the distance of points in different classes is the maximum. Strength: robustness; weakness: scalability.\n",
    "\n",
    "- Hidden Markov model: statistical model which assumes an underlying Markov process that generates the data.\n",
    "\n",
    "- Conditional Random Fields: classification algorithm used for sequential data; classifies elements individually.\n",
    "\n",
    "**Deep Learning for NLP**:\n",
    "\n",
    "- Recurrent Neural Networks (RNN): read and process input data sequentially; have short \"memory\".\n",
    "\n",
    "- Long Short-Term Memory (LSTM): a type of RNN; discards irrelevant context, only keeping the necessary part of it.\n",
    "\n",
    "- Convolutional Neural Networks (CNN): uses convolutions and pooling layers to represent text in a condensed manner; are able to analyze groups of words.\n",
    "\n",
    "- Transformers: model textual context non-sequentially, rather though [self-attention](https://towardsdatascience.com/understand-self-attention-in-bert-intuitively-cd480cbff30b); are thus granted with higher representation capacity. E.g., [BERT](https://huggingface.co/transformers/model_doc/bert.html).\n",
    "\n",
    "- Autoencoders: p. 90"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
